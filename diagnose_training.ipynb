{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from functools import partial\n",
    "import time\n",
    "import warnings\n",
    "from math import pi, inf\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import jax.debug as jdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args_class:\n",
    "    def __init__(self, seed, M, pnorm_init, p_freq, output_dir, use_x64):\n",
    "        self.seed = seed\n",
    "        self.M = M\n",
    "        self.pnorm_init = pnorm_init\n",
    "        self.p_freq = p_freq\n",
    "        self.output_dir = output_dir\n",
    "        self.use_x64 = use_x64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use argparse inside of ipynb\n",
    "args = args_class(0, 20, 2.2, 100, 'diagnose_training_files', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax                                          # noqa: E402\n",
    "import jax.numpy as jnp                             # noqa: E402\n",
    "from jax.example_libraries import optimizers             # noqa: E402\n",
    "from dynamics import prior                          # noqa: E402\n",
    "from utils import (tree_normsq, rk38_step, epoch,   # noqa: E402\n",
    "                   odeint_fixed_step, random_ragged_spline, spline,\n",
    "            params_to_cholesky, params_to_posdef)\n",
    "\n",
    "import jax.debug as jdebug\n",
    "\n",
    "def convert_p_qbar(p):\n",
    "    return jnp.sqrt(1/(1 - 1/p) - 1.1)\n",
    "\n",
    "def convert_qbar_p(qbar):\n",
    "    return 1/(1 - 1/(1.1 + qbar**2))\n",
    "\n",
    "# Initialize PRNG key\n",
    "key = jax.random.PRNGKey(args.seed)\n",
    "\n",
    "# Hyperparameters\n",
    "hparams = {\n",
    "    'seed':        args.seed,     #\n",
    "    'use_x64':     args.use_x64,  #\n",
    "    'num_subtraj': args.M,        # number of trajectories to sub-sample\n",
    "\n",
    "    # For training the model ensemble\n",
    "    'ensemble': {\n",
    "        'num_hlayers':    2,     # number of hidden layers in each model\n",
    "        'hdim':           32,    # number of hidden units per layer\n",
    "        'train_frac':     0.75,  # fraction of each trajectory for training\n",
    "        'batch_frac':     0.25,  # fraction of training data per batch\n",
    "        'regularizer_l2': 1e-4,  # coefficient for L2-regularization\n",
    "        'learning_rate':  1e-2,  # step size for gradient optimization\n",
    "        'num_epochs':     1000,  # number of epochs\n",
    "    },\n",
    "    # For meta-training\n",
    "    'meta': {\n",
    "        'num_hlayers':       2,          # number of hidden layers\n",
    "        'hdim':              32,         # number of hidden units per layer\n",
    "        'train_frac':        0.75,       #\n",
    "        'learning_rate':     1e-2,       # step size for gradient optimization\n",
    "        'num_steps':         1500,        # maximum number of gradient steps\n",
    "        'regularizer_l2':    1e-4,       # coefficient for L2-regularization\n",
    "        'regularizer_ctrl':  1e-3,       #\n",
    "        'regularizer_error': 0.,         #\n",
    "        'T':                 5.,         # time horizon for each reference\n",
    "        'dt':                1e-2,       # time step for numerical integration\n",
    "        'num_refs':          10,         # reference trajectories to generate\n",
    "        'num_knots':         6,          # knot points per reference spline\n",
    "        'poly_orders':       (9, 9, 6),  # spline orders for each DOF\n",
    "        'deriv_orders':      (4, 4, 2),  # smoothness objective for each DOF\n",
    "        'min_step':          (-2., -2., -pi/6),    #\n",
    "        'max_step':          (2., 2., pi/6),       #\n",
    "        'min_ref':           (-inf, -inf, -pi/3),  #\n",
    "        'max_ref':           (inf, inf, pi/3),     #\n",
    "        'p_freq':            args.p_freq,          # frequency for p-norm update\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_p_qbar(p):\n",
    "    return jnp.sqrt(1/(1 - 1/p) - 1.1)\n",
    "\n",
    "def convert_qbar_p(qbar):\n",
    "    return 1/(1 - 1/(1.1 + qbar**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PROCESSING ########################################################\n",
    "# Load raw data and arrange in samples of the form\n",
    "# `(t, x, u, t_next, x_next)` for each trajectory, where `x := (q,dq)`\n",
    "with open('training_data.pkl', 'rb') as file:\n",
    "    raw = pickle.load(file)\n",
    "num_dof = raw['q'].shape[-1]       # number of degrees of freedom\n",
    "num_traj = raw['q'].shape[0]       # total number of raw trajectories\n",
    "num_samples = raw['t'].size - 1    # number of transitions per trajectory\n",
    "t = jnp.tile(raw['t'][:-1], (num_traj, 1))\n",
    "t_next = jnp.tile(raw['t'][1:], (num_traj, 1))\n",
    "x = jnp.concatenate((raw['q'][:, :-1], raw['dq'][:, :-1]), axis=-1)\n",
    "x_next = jnp.concatenate((raw['q'][:, 1:], raw['dq'][:, 1:]), axis=-1)\n",
    "u = raw['u'][:, :-1]\n",
    "data = {'t': t, 'x': x, 'u': u, 't_next': t_next, 'x_next': x_next}\n",
    "\n",
    "# Shuffle and sub-sample trajectories\n",
    "if hparams['num_subtraj'] > num_traj:\n",
    "    warnings.warn('Cannot sub-sample {:d} trajectories! '\n",
    "                    'Capping at {:d}.'.format(hparams['num_subtraj'],\n",
    "                                            num_traj))\n",
    "    hparams['num_subtraj'] = num_traj\n",
    "\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "shuffled_idx = jax.random.permutation(subkey, num_traj)\n",
    "hparams['subtraj_idx'] = shuffled_idx[:hparams['num_subtraj']]\n",
    "data = jax.tree_util.tree_map(\n",
    "    lambda a: jnp.take(a, hparams['subtraj_idx'], axis=0),\n",
    "    data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENSEMBLE TRAINING: Pre-compiling ... done (1.27 s)!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d8dc5cd5f2143eb897a96640c4871fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# MODEL ENSEMBLE TRAINING ################################################\n",
    "# Loss function along a trajectory\n",
    "def ode(x, t, u, params, prior=prior):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    num_dof = x.size // 2\n",
    "    q, dq = x[:num_dof], x[num_dof:]\n",
    "    H, C, g, B = prior(q, dq)\n",
    "\n",
    "    # Each model in the ensemble is a feed-forward neural network\n",
    "    # with zero output bias\n",
    "    f = x\n",
    "    for W, b in zip(params['W'], params['b']):\n",
    "        f = jnp.tanh(W@f + b)\n",
    "    f = params['A'] @ f\n",
    "    ddq = jax.scipy.linalg.solve(H, B@u + f - C@dq - g, assume_a='pos')\n",
    "    dx = jnp.concatenate((dq, ddq))\n",
    "    return dx\n",
    "\n",
    "def loss(params, regularizer, t, x, u, t_next, x_next, ode=ode):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    num_samples = t.size\n",
    "    dt = t_next - t\n",
    "    x_next_est = jax.vmap(rk38_step, (None, 0, 0, 0, 0, None))(\n",
    "        ode, dt, x, t, u, params\n",
    "    )\n",
    "    loss = (jnp.sum((x_next_est - x_next)**2)\n",
    "            + regularizer*tree_normsq(params)) / num_samples\n",
    "    return loss\n",
    "\n",
    "# Parallel updates for each model in the ensemble\n",
    "@partial(jax.jit, static_argnums=(4, 5))\n",
    "@partial(jax.vmap, in_axes=(None, 0, None, 0, None, None))\n",
    "def step(idx, opt_state, regularizer, batch, get_params, update_opt,\n",
    "            loss=loss):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    params = get_params(opt_state)\n",
    "    grads = jax.grad(loss, argnums=0)(params, regularizer, **batch)\n",
    "    opt_state = update_opt(idx, grads, opt_state)\n",
    "    return opt_state\n",
    "\n",
    "@jax.jit\n",
    "@jax.vmap\n",
    "def update_best_ensemble(old_params, old_loss, new_params, batch):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    new_loss = loss(new_params, 0., **batch)  # do not regularize\n",
    "    best_params = jax.tree_util.tree_map(\n",
    "        lambda x, y: jnp.where(new_loss < old_loss, x, y),\n",
    "        new_params,\n",
    "        old_params\n",
    "    )\n",
    "    best_loss = jnp.where(new_loss < old_loss, new_loss, old_loss)\n",
    "    return best_params, best_loss, new_loss\n",
    "\n",
    "# Initialize model parameters\n",
    "num_models = hparams['num_subtraj']  # one model per trajectory\n",
    "num_hlayers = hparams['ensemble']['num_hlayers']\n",
    "hdim = hparams['ensemble']['hdim']\n",
    "if num_hlayers >= 1:\n",
    "    shapes = [(hdim, 2*num_dof), ] + (num_hlayers-1)*[(hdim, hdim), ]\n",
    "else:\n",
    "    shapes = []\n",
    "key, *subkeys = jax.random.split(key, 1 + 2*num_hlayers + 1)\n",
    "keys_W = subkeys[:num_hlayers]\n",
    "keys_b = subkeys[num_hlayers:-1]\n",
    "key_A = subkeys[-1]\n",
    "ensemble = {\n",
    "    # hidden layer weights\n",
    "    'W': [0.1*jax.random.normal(keys_W[i], (num_models, *shapes[i]))\n",
    "            for i in range(num_hlayers)],\n",
    "    # hidden layer biases\n",
    "    'b': [0.1*jax.random.normal(keys_b[i], (num_models, shapes[i][0]))\n",
    "            for i in range(num_hlayers)],\n",
    "    # last layer weights\n",
    "    'A': 0.1*jax.random.normal(key_A, (num_models, num_dof, hdim))\n",
    "}\n",
    "\n",
    "# Shuffle samples in time along each trajectory, then split each\n",
    "# trajectory into training and validation sets (i.e., for each model)\n",
    "key, *subkeys = jax.random.split(key, 1 + num_models)\n",
    "subkeys = jnp.asarray(subkeys)\n",
    "shuffled_data = jax.tree_util.tree_map(\n",
    "    lambda a: jax.vmap(jax.random.permutation)(subkeys, a),\n",
    "    data\n",
    ")\n",
    "num_train_samples = int(hparams['ensemble']['train_frac'] * num_samples)\n",
    "ensemble_train_data = jax.tree_util.tree_map(\n",
    "    lambda a: a[:, :num_train_samples],\n",
    "    shuffled_data\n",
    ")\n",
    "ensemble_valid_data = jax.tree_util.tree_map(\n",
    "    lambda a: a[:, num_train_samples:],\n",
    "    shuffled_data\n",
    ")\n",
    "\n",
    "# Initialize gradient-based optimizer (ADAM)\n",
    "learning_rate = hparams['ensemble']['learning_rate']\n",
    "batch_size = int(hparams['ensemble']['batch_frac'] * num_train_samples)\n",
    "num_batches = num_train_samples // batch_size\n",
    "init_opt, update_opt, get_params = optimizers.adam(learning_rate)\n",
    "opt_states = jax.vmap(init_opt)(ensemble)\n",
    "get_ensemble = jax.jit(jax.vmap(get_params))\n",
    "step_idx = 0\n",
    "best_idx = jnp.zeros(num_models)\n",
    "\n",
    "# Pre-compile before training\n",
    "print('ENSEMBLE TRAINING: Pre-compiling ... ', end='', flush=True)\n",
    "start = time.time()\n",
    "batch = next(epoch(key, ensemble_train_data, batch_size,\n",
    "                    batch_axis=1, ragged=False))\n",
    "_ = step(step_idx, opt_states, hparams['ensemble']['regularizer_l2'],\n",
    "            batch, get_params, update_opt)\n",
    "inf_losses = jnp.broadcast_to(jnp.inf, (num_models,))\n",
    "best_ensemble, best_losses, _ = update_best_ensemble(ensemble,\n",
    "                                                        inf_losses,\n",
    "                                                        ensemble,\n",
    "                                                        ensemble_valid_data)\n",
    "_ = get_ensemble(opt_states)\n",
    "end = time.time()\n",
    "print('done ({:.2f} s)!'.format(end - start))\n",
    "\n",
    "# Do gradient descent\n",
    "for _ in tqdm(range(hparams['ensemble']['num_epochs'])):\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    for batch in epoch(subkey, ensemble_train_data, batch_size,\n",
    "                        batch_axis=1, ragged=False):\n",
    "        opt_states = step(step_idx, opt_states,\n",
    "                            hparams['ensemble']['regularizer_l2'],\n",
    "                            batch, get_params, update_opt)\n",
    "        new_ensemble = get_ensemble(opt_states)\n",
    "        old_losses = best_losses\n",
    "        best_ensemble, best_losses, valid_losses = update_best_ensemble(\n",
    "            best_ensemble, best_losses, new_ensemble, batch\n",
    "        )\n",
    "        step_idx += 1\n",
    "        best_idx = jnp.where(old_losses == best_losses,\n",
    "                                best_idx, step_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize pnorm as 2.20\n",
      "META-TRAINING: Pre-compiling ... done (12.28 s)! Now training ...\n",
      "seed=0_M=10_pinit=2.200000_pfreq=100.000000_reg_gain\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1db5b0fd0154026b7e164e615e4790a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update p-norm to 2.23 at step 99\n",
      "Update p-norm to 2.25 at step 199\n",
      "Update p-norm to 2.28 at step 299\n",
      "Update p-norm to 2.30 at step 399\n",
      "Update p-norm to 2.32 at step 499\n",
      "Update p-norm to 2.34 at step 599\n",
      "Update p-norm to 2.36 at step 699\n",
      "Update p-norm to 2.38 at step 799\n",
      "Update p-norm to 2.40 at step 899\n",
      "Update p-norm to 2.41 at step 999\n",
      "Update p-norm to 2.42 at step 1099\n",
      "Update p-norm to 2.43 at step 1199\n",
      "Update p-norm to 2.44 at step 1299\n",
      "Update p-norm to 2.45 at step 1399\n",
      "Update p-norm to 2.46 at step 1499\n",
      "Meta-training completes with p-norm chosen as 2.46\n",
      "done (1135.59 s)! Best step index for meta params: 1499\n"
     ]
    }
   ],
   "source": [
    "from jax import config\n",
    "config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "# META-TRAINING ##########################################################\n",
    "def ode(z, t, meta_params, pnorm_param, params, reference, prior=prior):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    x, pA, c = z\n",
    "    num_dof = x.size // 2\n",
    "    q, dq = x[:num_dof], x[num_dof:]\n",
    "    r = reference(t)\n",
    "    dr = jax.jacfwd(reference)(t)\n",
    "    ddr = jax.jacfwd(jax.jacfwd(reference))(t)\n",
    "\n",
    "    # Regressor features\n",
    "    y = x\n",
    "    for W, b in zip(meta_params['W'], meta_params['b']):\n",
    "        y = jnp.tanh(W@y + b)\n",
    "\n",
    "    # Parameterized control and adaptation gains\n",
    "    gains = jax.tree_util.tree_map(\n",
    "        lambda x: params_to_posdef(x),\n",
    "        meta_params['gains']\n",
    "    )\n",
    "    Λ, K, P = gains['Λ'], gains['K'], gains['P']\n",
    "\n",
    "    qn = 1.1 + pnorm_param['pnorm']**2\n",
    "\n",
    "    A = (jnp.maximum(jnp.abs(pA), 1e-6 * jnp.ones_like(pA))**(qn-1) * jnp.sign(pA) * (jnp.ones_like(pA) - jnp.isclose(pA, 0, atol=1e-6))) @ P\n",
    "    # A = (jnp.maximum(jnp.abs(pA), 1e-6 * jnp.ones_like(pA))**(qn-1) * jnp.sign(pA) * jnp.isclose(pA, 0, atol=1e-6)) @ P\n",
    "    #A = pA\n",
    "\n",
    "    # Auxiliary signals\n",
    "    e, de = q - r, dq - dr\n",
    "    v, dv = dr - Λ@e, ddr - Λ@de\n",
    "    s = de + Λ@e\n",
    "\n",
    "    # Controller and adaptation law\n",
    "    H, C, g, B = prior(q, dq)\n",
    "    f_hat = A@y\n",
    "    τ = H@dv + C@v + g - f_hat - K@s\n",
    "    u = jnp.linalg.solve(B, τ)\n",
    "    dpA = jnp.outer(s, y) @ P\n",
    "\n",
    "    # Apply control to \"true\" dynamics\n",
    "    f = x\n",
    "    for W, b in zip(params['W'], params['b']):\n",
    "        f = jnp.tanh(W@f + b)\n",
    "    f = params['A'] @ f\n",
    "    ddq = jax.scipy.linalg.solve(H, τ + f - C@dq - g, assume_a='pos')\n",
    "    dx = jnp.concatenate((dq, ddq))\n",
    "\n",
    "    # Estimation loss\n",
    "    # chol_P = params_to_cholesky(meta_params['gains']['P'])\n",
    "    # f_error = f_hat - f\n",
    "    # loss_est = f_error@jax.scipy.linalg.cho_solve((chol_P, True),\n",
    "    #                                               f_error)\n",
    "\n",
    "    # Integrated cost terms\n",
    "    dc = jnp.array([\n",
    "        e@e + de@de,                # tracking loss\n",
    "        u@u,                        # control loss\n",
    "        (f_hat - f)@(f_hat - f),    # estimation loss\n",
    "    ])\n",
    "\n",
    "    # Assemble derivatives\n",
    "    dz = (dx, dpA, dc)\n",
    "    return dz\n",
    "\n",
    "# Simulate adaptive control loop on each model in the ensemble\n",
    "def ensemble_sim(meta_params, pnorm_param, ensemble_params, reference, T, dt, ode=ode):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    # Initial conditions\n",
    "    r0 = reference(0.)\n",
    "    dr0 = jax.jacfwd(reference)(0.)\n",
    "    num_dof = r0.size\n",
    "    num_features = meta_params['W'][-1].shape[0]\n",
    "    x0 = jnp.concatenate((r0, dr0))\n",
    "    A0 = jnp.zeros((num_dof, num_features))\n",
    "    c0 = jnp.zeros(3)\n",
    "    z0 = (x0, A0, c0)\n",
    "\n",
    "    # Integrate the adaptive control loop using the meta-model\n",
    "    # and EACH model in the ensemble along the same reference\n",
    "    in_axes = (None, None, None, None, None, None, None, 0)\n",
    "    ode = partial(ode, reference=reference)\n",
    "    z, t = jax.vmap(odeint_fixed_step, in_axes)(ode, z0, 0., T, dt,\n",
    "                                                meta_params, pnorm_param,\n",
    "                                                ensemble_params)\n",
    "    x, A, c = z\n",
    "    return t, x, A, c\n",
    "\n",
    "# Initialize meta-model parameters\n",
    "num_hlayers = hparams['meta']['num_hlayers']\n",
    "hdim = hparams['meta']['hdim']\n",
    "if num_hlayers >= 1:\n",
    "    shapes = [(hdim, 2*num_dof), ] + (num_hlayers-1)*[(hdim, hdim), ]\n",
    "else:\n",
    "    shapes = []\n",
    "key, *subkeys = jax.random.split(key, 1 + 2*num_hlayers + 3)\n",
    "subkeys_W = subkeys[:num_hlayers]\n",
    "subkeys_b = subkeys[num_hlayers:-3]\n",
    "subkeys_gains = subkeys[-3:]\n",
    "meta_params = {\n",
    "    # hidden layer weights\n",
    "    'W': [0.1*jax.random.normal(subkeys_W[i], shapes[i])\n",
    "            for i in range(num_hlayers)],\n",
    "    # hidden layer biases\n",
    "    'b': [0.1*jax.random.normal(subkeys_b[i], (shapes[i][0],))\n",
    "            for i in range(num_hlayers)],\n",
    "    'gains': {  # vectorized control and adaptation gains\n",
    "        'Λ': 0.1*jax.random.normal(subkeys_gains[0],\n",
    "                                    ((num_dof*(num_dof + 1)) // 2,)),\n",
    "        'K': 0.1*jax.random.normal(subkeys_gains[1],\n",
    "                                    ((num_dof*(num_dof + 1)) // 2,)),\n",
    "        'P': 0.1*jax.random.normal(subkeys_gains[2],\n",
    "                                    ((hdim*(hdim + 1)) // 2,)),\n",
    "    },\n",
    "}\n",
    "# In the bash script, we always specify p-norm desried initial values\n",
    "# Note that the program always uses the q_bar parameter as the p-norm parameterization\n",
    "# However, the printing function should log the final results in p-norm\n",
    "pnorm_param = {'pnorm': convert_p_qbar(args.pnorm_init)}\n",
    "print(\"Initialize pnorm as {:.2f}\".format(convert_qbar_p(pnorm_param['pnorm'])))\n",
    "\n",
    "# Initialize spline coefficients for each reference trajectory\n",
    "num_refs = hparams['meta']['num_refs']\n",
    "key, *subkeys = jax.random.split(key, 1 + num_refs)\n",
    "subkeys = jnp.vstack(subkeys)\n",
    "in_axes = (0, None, None, None, None, None, None, None, None)\n",
    "min_ref = jnp.asarray(hparams['meta']['min_ref'])\n",
    "max_ref = jnp.asarray(hparams['meta']['max_ref'])\n",
    "t_knots, knots, coefs = jax.vmap(random_ragged_spline, in_axes)(\n",
    "    subkeys,\n",
    "    hparams['meta']['T'],\n",
    "    hparams['meta']['num_knots'],\n",
    "    hparams['meta']['poly_orders'],\n",
    "    hparams['meta']['deriv_orders'],\n",
    "    jnp.asarray(hparams['meta']['min_step']),\n",
    "    jnp.asarray(hparams['meta']['max_step']),\n",
    "    0.7*min_ref,\n",
    "    0.7*max_ref,\n",
    ")\n",
    "# x_coefs, y_coefs, θ_coefs = coefs\n",
    "# x_knots, y_knots, θ_knots = knots\n",
    "r_knots = jnp.dstack(knots)\n",
    "\n",
    "# Simulate the adaptive control loop for each model in the ensemble and\n",
    "# each reference trajectory (i.e., spline coefficients)\n",
    "@partial(jax.vmap, in_axes=(None, None, None, 0, 0, None, None))\n",
    "def simulate(meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt, min_ref=min_ref, max_ref=max_ref):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    # Define a reference trajectory in terms of spline coefficients\n",
    "    def reference(t):\n",
    "        r = jnp.array([spline(t, t_knots, c) for c in coefs])\n",
    "        r = jnp.clip(r, min_ref, max_ref)\n",
    "        return r\n",
    "    t, x, A, c = ensemble_sim(meta_params, pnorm_param, ensemble_params,\n",
    "                                reference, T, dt)\n",
    "    return t, x, A, c\n",
    "\n",
    "@partial(jax.jit, static_argnums=(5, 6))\n",
    "def loss(meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt,\n",
    "            regularizer_l2, regularizer_ctrl, regularizer_error):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    # Simulate on each model for each reference trajectory\n",
    "    t, x, A, c = simulate(meta_params, pnorm_param, ensemble_params, t_knots,\n",
    "                            coefs, T, dt)\n",
    "\n",
    "    # Sum final costs over reference trajectories and ensemble models\n",
    "    # Note `c` has shape (`num_refs`, `num_models`, `T // dt`, 3)\n",
    "    c_final = jnp.sum(c[:, :, -1, :], axis=(0, 1))\n",
    "\n",
    "    # Form a composite loss by weighting the different cost integrals,\n",
    "    # and normalizing by the number of models, number of reference\n",
    "    # trajectories, and time horizon\n",
    "    num_refs = c.shape[0]\n",
    "    num_models = c.shape[1]\n",
    "    normalizer = T * num_refs * num_models\n",
    "    tracking_loss, control_loss, estimation_loss = c_final\n",
    "    reg_gain = jnp.linalg.norm(meta_params['gains']['P'])**2\n",
    "    l2_penalty = tree_normsq((meta_params['W'], meta_params['b']))\n",
    "    loss = (tracking_loss\n",
    "            + regularizer_ctrl*control_loss\n",
    "            + regularizer_error*estimation_loss\n",
    "            + regularizer_l2*l2_penalty\n",
    "            + 5 * reg_gain) / normalizer\n",
    "    aux = {\n",
    "        # for each model in ensemble\n",
    "        'tracking_loss':   jnp.sum(c[:, :, -1, 0], axis=0) / num_refs,\n",
    "        'control_loss':    jnp.sum(c[:, :, -1, 1], axis=0) / num_refs,\n",
    "        'estimation_loss': jnp.sum(c[:, :, -1, 2], axis=0) / num_refs,\n",
    "        'l2_penalty':      l2_penalty,\n",
    "        'eigs_Λ':\n",
    "            jnp.diag(params_to_cholesky(meta_params['gains']['Λ']))**2,\n",
    "        'eigs_K':\n",
    "            jnp.diag(params_to_cholesky(meta_params['gains']['K']))**2,\n",
    "        'eigs_P':\n",
    "            jnp.diag(params_to_cholesky(meta_params['gains']['P']))**2,\n",
    "        'pnorm': pnorm_param['pnorm']\n",
    "    }\n",
    "    return loss, aux\n",
    "\n",
    "# Shuffle and split ensemble into training and validation sets\n",
    "train_frac = hparams['meta']['train_frac']\n",
    "num_train_models = int(train_frac * num_models)\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "model_idx = jax.random.permutation(subkey, num_models)\n",
    "train_model_idx = model_idx[:num_train_models]\n",
    "valid_model_idx = model_idx[num_train_models:]\n",
    "train_ensemble = jax.tree_util.tree_map(lambda x: x[train_model_idx],\n",
    "                                        best_ensemble)\n",
    "valid_ensemble = jax.tree_util.tree_map(lambda x: x[valid_model_idx],\n",
    "                                        best_ensemble)\n",
    "\n",
    "# Split reference trajectories into training and validation sets\n",
    "num_train_refs = int(train_frac * num_refs)\n",
    "train_t_knots = jax.tree_util.tree_map(lambda a: a[:num_train_refs],\n",
    "                                        t_knots)\n",
    "train_coefs = jax.tree_util.tree_map(lambda a: a[:num_train_refs], coefs)\n",
    "valid_t_knots = jax.tree_util.tree_map(lambda a: a[num_train_refs:],\n",
    "                                        t_knots)\n",
    "valid_coefs = jax.tree_util.tree_map(lambda a: a[num_train_refs:], coefs)\n",
    "\n",
    "# Initialize gradient-based optimizer (ADAM)\n",
    "learning_rate = hparams['meta']['learning_rate']\n",
    "init_opt, update_opt, get_params = optimizers.adam(learning_rate)\n",
    "# Update meta_params and pnorm_param separately\n",
    "opt_meta = init_opt(meta_params)\n",
    "opt_pnorm = init_opt(pnorm_param)\n",
    "step_meta_idx = 0\n",
    "step_pnorm_idx = 0\n",
    "best_idx_meta = 0\n",
    "best_idx_pnorm = 0\n",
    "best_loss = jnp.inf\n",
    "best_meta_params = meta_params\n",
    "best_pnorm_param = pnorm_param\n",
    "\n",
    "@partial(jax.jit, static_argnums=(6, 7))\n",
    "def step_meta(idx, opt_state, pnorm_param, ensemble_params, t_knots, coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error):\n",
    "    \"\"\"This function only updates the meta_params in an iteration\"\"\"\n",
    "    meta_params = get_params(opt_state)\n",
    "    grads, aux = jax.grad(loss, argnums=0, has_aux=True)(\n",
    "        meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt,\n",
    "        regularizer_l2, regularizer_ctrl, regularizer_error\n",
    "    )\n",
    "    #isnan_Wgrad = jnp.any(jnp.isnan(grads['W'][0]))\n",
    "    #isnan_bgrad = jnp.any(jnp.isnan(grads['b'][0]))\n",
    "    # jdb.print('{isnan_Wgrad}', isnan_Wgrad=isnan_Wgrad)\n",
    "    # jdb.print('{isnan_bgrad}', isnan_bgrad=isnan_bgrad)\n",
    "    # if isnan_Wgrad:\n",
    "    #     jdb.print(\"W[0] gradient is nan!\")\n",
    "    #     raise ValueError(\"W[0] gradient is nan!\")\n",
    "    # elif isnan_bgrad:\n",
    "    #     jdb.print(\"b[0] gradient is nan!\")\n",
    "    #     raise ValueError(\"b[0] gradient is nan!\")\n",
    "    opt_state = update_opt(idx, grads, opt_state)\n",
    "    return opt_state, aux, grads\n",
    "\n",
    "# @partial(jax.jit, static_argnums=(6, 7))\n",
    "def step_pnorm(idx, meta_params, opt_state, ensemble_params, t_knots, coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error):\n",
    "    \"\"\"This function only updates the meta_params in an iteration\"\"\"\n",
    "    pnorm_param = get_params(opt_state)\n",
    "    grads, aux = jax.grad(loss, argnums=1, has_aux=True)(\n",
    "        meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt,\n",
    "        regularizer_l2, regularizer_ctrl, regularizer_error\n",
    "    )\n",
    "    opt_state = update_opt(idx, grads, opt_state)\n",
    "    return opt_state, aux, grads\n",
    "\n",
    "# Pre-compile before training\n",
    "print('META-TRAINING: Pre-compiling ... ', end='', flush=True)\n",
    "dt = hparams['meta']['dt']\n",
    "T = hparams['meta']['T']\n",
    "regularizer_l2 = hparams['meta']['regularizer_l2']\n",
    "regularizer_ctrl = hparams['meta']['regularizer_ctrl']\n",
    "regularizer_error = hparams['meta']['regularizer_error']\n",
    "start = time.time()\n",
    "_ = step_meta(0, opt_meta, pnorm_param, train_ensemble, train_t_knots, train_coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error)\n",
    "_ = step_pnorm(0, meta_params, opt_pnorm, train_ensemble, train_t_knots, train_coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error)\n",
    "_ = loss(meta_params, pnorm_param, valid_ensemble, valid_t_knots, valid_coefs, T, dt,\n",
    "            0., 0., 0.)\n",
    "end = time.time()\n",
    "print('done ({:.2f} s)! Now training ...'.format(\n",
    "        end - start))\n",
    "start = time.time()\n",
    "\n",
    "output_name = \"seed={:d}_M={:d}_pinit={:f}_pfreq={:f}_reg_gain\".format(hparams['seed'], num_models, args.pnorm_init, hparams['meta']['p_freq'])\n",
    "print(output_name)\n",
    "\n",
    "# Do gradient descent\n",
    "for i in tqdm(range(hparams['meta']['num_steps'])):\n",
    "    opt_meta, train_aux_meta, grads_meta = step_meta(\n",
    "        step_meta_idx, opt_meta, pnorm_param, train_ensemble, train_t_knots, train_coefs,\n",
    "        T, dt, regularizer_l2, regularizer_ctrl, regularizer_error\n",
    "    )\n",
    "    # print(train_aux_meta)\n",
    "    new_meta_params = get_params(opt_meta)\n",
    "\n",
    "    # Update p-norm parameter\n",
    "    # The i+1 is to make sure not to update p-norm at step 0\n",
    "    if (i+1) % hparams['meta']['p_freq'] == 0:\n",
    "        opt_pnorm, train_aux_pnorm, grads_pnorm = step_pnorm(\n",
    "            step_pnorm_idx, new_meta_params, opt_pnorm, train_ensemble, train_t_knots, train_coefs,\n",
    "            T, dt, regularizer_l2, regularizer_ctrl, regularizer_error\n",
    "        )\n",
    "        step_pnorm_idx += 1\n",
    "        # jdebug.print('{grad_pnorm}', grad_pnorm=grads_pnorm)\n",
    "        # jdebug.print('{meta_grad}', meta_grad=grads_meta)\n",
    "        print(\"Update p-norm to {:.2f} at step {:d}\".format(convert_qbar_p(get_params(opt_pnorm)['pnorm']), step_meta_idx))\n",
    "    new_pnorm_param = get_params(opt_pnorm)\n",
    "\n",
    "    valid_loss, valid_aux = loss(\n",
    "        new_meta_params, new_pnorm_param, valid_ensemble, valid_t_knots, valid_coefs,\n",
    "        T, dt, 0., 0., 0.\n",
    "    )\n",
    "    #jdb.print(f'{valid_loss}')\n",
    "\n",
    "    # Only update best_meta_params when the loss is decreasing\n",
    "    if valid_loss < best_loss:\n",
    "        best_meta_params = new_meta_params\n",
    "        best_pnorm_param = new_pnorm_param\n",
    "        best_loss = valid_loss\n",
    "        best_idx_meta = step_meta_idx\n",
    "        best_idx_pnorm = step_pnorm_idx\n",
    "    step_meta_idx += 1\n",
    "    # print(step_meta_idx)\n",
    "\n",
    "# Save hyperparameters, ensemble, model, and controller\n",
    "\n",
    "results = {\n",
    "    'best_step_meta': best_idx_meta,\n",
    "    'best_step_pnorm': best_idx_pnorm,\n",
    "    'hparams': hparams,\n",
    "    'ensemble': best_ensemble,\n",
    "    'model': {\n",
    "        'W': best_meta_params['W'],\n",
    "        'b': best_meta_params['b'],\n",
    "    },\n",
    "    'controller': best_meta_params['gains'],\n",
    "    'pnorm': convert_qbar_p(best_pnorm_param['pnorm']),\n",
    "}\n",
    "output_path = os.path.join(args.output_dir, output_name + '.pkl')\n",
    "with open(output_path, 'wb') as file:\n",
    "    pickle.dump(results, file)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Meta-training completes with p-norm chosen as {:.2f}\".format(results['pnorm']))\n",
    "print('done ({:.2f} s)! Best step index for meta params: {}'.format(end - start, best_idx_meta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
