{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josh/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "TODO description.\n",
    "\n",
    "Author: Spencer M. Richards\n",
    "        Autonomous Systems Lab (ASL), Stanford\n",
    "        (GitHub: spenrich)\n",
    "\"\"\"\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "from functools import partial\n",
    "import time\n",
    "import warnings\n",
    "from math import pi, inf\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jax import config\n",
    "config.update(\"jax_debug_nans\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args_class:\n",
    "    def __init__(self, seed, M, pnorm_init, p_freq, meta_epochs, reg_P, output_dir, use_x64):\n",
    "        self.seed = seed\n",
    "        self.M = M\n",
    "        self.pnorm_init = pnorm_init\n",
    "        self.p_freq = p_freq\n",
    "        self.meta_epochs = meta_epochs\n",
    "        self.reg_P = reg_P\n",
    "        self.output_dir = output_dir\n",
    "        self.use_x64 = use_x64\n",
    "\n",
    "args = args_class(0, 25, 2.0, 50, 4000, 2e-3, 'diagnose_regP', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Parse command line arguments\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('seed', help='seed for pseudo-random number generation',\n",
    "#                     type=int)\n",
    "# parser.add_argument('M', help='number of trajectories to sub-sample',\n",
    "#                     type=int)\n",
    "# parser.add_argument('--use_x64', help='use 64-bit precision',\n",
    "#                     action='store_true')\n",
    "# parser.add_argument('--pnorm_init', help='set initial value for p-norm choices', type=float)\n",
    "# parser.add_argument('--p_freq', help='set frequency for p-norm parameter update', type=float)\n",
    "# parser.add_argument('--meta_epochs', help='set number of epochs for meta-training', type=int)\n",
    "# parser.add_argument('--reg_P', help='set regularization for P matrix', type=float)\n",
    "# parser.add_argument('--output_dir', help='set output directory', type=str)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Set precision\n",
    "if args.use_x64:\n",
    "    os.environ['JAX_ENABLE_X64'] = 'True'\n",
    "\n",
    "import jax                                          # noqa: E402\n",
    "import jax.numpy as jnp                             # noqa: E402\n",
    "from jax.example_libraries import optimizers             # noqa: E402\n",
    "from dynamics import prior                          # noqa: E402\n",
    "from utils import (tree_normsq, rk38_step, epoch,   # noqa: E402\n",
    "                   odeint_fixed_step, random_ragged_spline, spline,\n",
    "            params_to_cholesky, params_to_posdef, \n",
    "            quaternion_to_rotation_matrix, hat, vee)\n",
    "\n",
    "import jax.debug as jdebug\n",
    "\n",
    "def convert_p_qbar(p):\n",
    "    return jnp.sqrt(1/(1 - 1/p) - 1.1)\n",
    "\n",
    "def convert_qbar_p(qbar):\n",
    "    return 1/(1 - 1/(1.1 + qbar**2))\n",
    "\n",
    "# Initialize PRNG key\n",
    "key = jax.random.PRNGKey(args.seed)\n",
    "\n",
    "# Hyperparameters\n",
    "hparams = {\n",
    "    'seed':        args.seed,     #\n",
    "    'use_x64':     args.use_x64,  #\n",
    "    'num_subtraj': args.M,        # number of trajectories to sub-sample\n",
    "\n",
    "    # For training the model ensemble\n",
    "    'ensemble': {\n",
    "        'num_hlayers':    2,     # number of hidden layers in each model\n",
    "        'hdim':           32,    # number of hidden units per layer\n",
    "        'train_frac':     0.75,  # fraction of each trajectory for training\n",
    "        'batch_frac':     0.25,  # fraction of training data per batch\n",
    "        'regularizer_l2': 1e-4,  # coefficient for L2-regularization\n",
    "        'learning_rate':  1e-2,  # step size for gradient optimization\n",
    "        'num_epochs':     1000,  # number of epochs\n",
    "    },\n",
    "    # For meta-training\n",
    "    'meta': {\n",
    "        'num_hlayers':       2,          # number of hidden layers\n",
    "        'hdim':              32,         # number of hidden units per layer\n",
    "        'train_frac':        0.75,       #\n",
    "        'learning_rate':     1e-2,       # step size for gradient optimization\n",
    "        'num_steps':         args.meta_epochs,        # maximum number of gradient steps\n",
    "        'regularizer_l2':    1e-4,       # coefficient for L2-regularization\n",
    "        'regularizer_ctrl':  1e-3,       #\n",
    "        'regularizer_error': 0.,         #\n",
    "        'T':                 5.,         # time horizon for each reference\n",
    "        'dt':                1e-2,       # time step for numerical integration\n",
    "        'num_refs':          10,         # reference trajectories to generate\n",
    "        'num_knots':         6,          # knot points per reference spline\n",
    "        'poly_orders':       (9, 9, 9),  # spline orders for each DOF\n",
    "        'deriv_orders':      (4, 4, 4),  # smoothness objective for each DOF\n",
    "        'min_step':          (-2., -2., -0.75),    #\n",
    "        'max_step':          (2., 2., 0.75),       #\n",
    "        'min_ref':           (-inf, -inf, -inf),  #\n",
    "        'max_ref':           (inf, inf, inf),     #\n",
    "        'p_freq':            args.p_freq,          # frequency for p-norm update\n",
    "        'regularizer_P':     args.reg_P,           # coefficient for P regularization\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PROCESSING ########################################################\n",
    "# Load raw data and arrange in samples of the form\n",
    "# `(t, x, u, t_next, x_next)` for each trajectory, where `x := (q,dq)`\n",
    "with open('data/2024-04-04_23-51-17_traj50_seed0.pkl', 'rb') as file:\n",
    "    raw = pickle.load(file)\n",
    "num_dof = raw['q'].shape[-1]       # number of degrees of freedom\n",
    "param_dim = 2*num_dof + 9 + 3    # number of degrees of freedom including attitude (9 for rotation matrix, 3 for angular velocity)\n",
    "num_traj = raw['q'].shape[0]       # total number of raw trajectories\n",
    "num_samples = raw['t'].size - 1    # number of transitions per trajectory\n",
    "t = jnp.tile(raw['t'][:-1], (num_traj, 1))\n",
    "t_next = jnp.tile(raw['t'][1:], (num_traj, 1))\n",
    "x = jnp.concatenate((raw['q'][:, :-1], raw['dq'][:, :-1]), axis=-1)\n",
    "x_next = jnp.concatenate((raw['q'][:, 1:], raw['dq'][:, 1:]), axis=-1)\n",
    "u = raw['u'][:, :-1, :3]\n",
    "quat = raw['quat'][:, :-1]\n",
    "R = jax.vmap(jax.vmap(quaternion_to_rotation_matrix, in_axes=0), in_axes=0)(quat)\n",
    "R_flatten = R.reshape(R.shape[0], R.shape[1], -1)\n",
    "omega = raw['omega'][:, :-1]\n",
    "data = {'t': t, 'x': x, 'u': u, 'R_flatten': R_flatten, 'omega': omega, 't_next': t_next, 'x_next': x_next}\n",
    "\n",
    "# Shuffle and sub-sample trajectories\n",
    "if hparams['num_subtraj'] > num_traj:\n",
    "    warnings.warn('Cannot sub-sample {:d} trajectories! '\n",
    "                    'Capping at {:d}.'.format(hparams['num_subtraj'],\n",
    "                                            num_traj))\n",
    "    hparams['num_subtraj'] = num_traj\n",
    "\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "shuffled_idx = jax.random.permutation(subkey, num_traj)\n",
    "hparams['subtraj_idx'] = shuffled_idx[:hparams['num_subtraj']]\n",
    "data = jax.tree_util.tree_map(\n",
    "    lambda a: jnp.take(a, hparams['subtraj_idx'], axis=0),\n",
    "    data\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENSEMBLE TRAINING: Pre-compiling ... done (0.93 s)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [12:34<00:00,  1.33it/s]\n"
     ]
    }
   ],
   "source": [
    "# MODEL ENSEMBLE TRAINING ################################################\n",
    "# Loss function along a trajectory\n",
    "def ode(x, R_flatten, omega, t, u, params, prior=prior):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    num_dof = x.size // 2\n",
    "    q, dq = x[:num_dof], x[num_dof:]\n",
    "    H, C, g, B = prior(q, dq)\n",
    "\n",
    "    # Each model in the ensemble is a feed-forward neural network\n",
    "    # with zero output bias\n",
    "    f_ext = x\n",
    "    f_ext = jnp.concatenate([f_ext, R_flatten, omega], axis=0)\n",
    "    for W, b in zip(params['W'], params['b']):\n",
    "        f_ext = jnp.tanh(W@f_ext + b)\n",
    "    f_ext = params['A'] @ f_ext\n",
    "    ddq = jax.scipy.linalg.solve(H, B@u + f_ext - C@dq - g, assume_a='pos')\n",
    "    dx = jnp.concatenate((dq, ddq))\n",
    "    return dx\n",
    "\n",
    "def loss(params, regularizer, t, x, R_flatten, omega, u, t_next, x_next, ode=ode):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    num_samples = t.size\n",
    "    dt = t_next - t\n",
    "    x_next_est = jax.vmap(rk38_step, (None, 0, 0, 0, 0, 0, 0, None))(\n",
    "        ode, dt, x, R_flatten, omega, t, u, params\n",
    "    )\n",
    "    loss = (jnp.sum((x_next_est - x_next)**2)\n",
    "            + regularizer*tree_normsq(params)) / num_samples\n",
    "    return loss\n",
    "\n",
    "# Parallel updates for each model in the ensemble\n",
    "@partial(jax.jit, static_argnums=(4, 5))\n",
    "@partial(jax.vmap, in_axes=(None, 0, None, 0, None, None))\n",
    "def step(idx, opt_state, regularizer, batch, get_params, update_opt,\n",
    "            loss=loss):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    params = get_params(opt_state)\n",
    "    grads = jax.grad(loss, argnums=0)(params, regularizer, **batch)\n",
    "    opt_state = update_opt(idx, grads, opt_state)\n",
    "    return opt_state\n",
    "\n",
    "@jax.jit\n",
    "@jax.vmap\n",
    "def update_best_ensemble(old_params, old_loss, new_params, batch):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    new_loss = loss(new_params, 0., **batch)  # do not regularize\n",
    "    best_params = jax.tree_util.tree_map(\n",
    "        lambda x, y: jnp.where(new_loss < old_loss, x, y),\n",
    "        new_params,\n",
    "        old_params\n",
    "    )\n",
    "    best_loss = jnp.where(new_loss < old_loss, new_loss, old_loss)\n",
    "    return best_params, best_loss, new_loss\n",
    "\n",
    "# Initialize model parameters\n",
    "num_models = hparams['num_subtraj']  # one model per trajectory\n",
    "num_hlayers = hparams['ensemble']['num_hlayers']\n",
    "hdim = hparams['ensemble']['hdim']\n",
    "if num_hlayers >= 1:\n",
    "    shapes = [(hdim, param_dim), ] + (num_hlayers-1)*[(hdim, hdim), ]\n",
    "else:\n",
    "    shapes = []\n",
    "key, *subkeys = jax.random.split(key, 1 + 2*num_hlayers + 1)\n",
    "keys_W = subkeys[:num_hlayers]\n",
    "keys_b = subkeys[num_hlayers:-1]\n",
    "key_A = subkeys[-1]\n",
    "ensemble = {\n",
    "    # hidden layer weights\n",
    "    'W': [0.1*jax.random.normal(keys_W[i], (num_models, *shapes[i]))\n",
    "            for i in range(num_hlayers)],\n",
    "    # hidden layer biases\n",
    "    'b': [0.1*jax.random.normal(keys_b[i], (num_models, shapes[i][0]))\n",
    "            for i in range(num_hlayers)],\n",
    "    # last layer weights\n",
    "    'A': 0.1*jax.random.normal(key_A, (num_models, num_dof, hdim))\n",
    "}\n",
    "\n",
    "# Shuffle samples in time along each trajectory, then split each\n",
    "# trajectory into training and validation sets (i.e., for each model)\n",
    "key, *subkeys = jax.random.split(key, 1 + num_models)\n",
    "subkeys = jnp.asarray(subkeys)\n",
    "shuffled_data = jax.tree_util.tree_map(\n",
    "    lambda a: jax.vmap(jax.random.permutation)(subkeys, a),\n",
    "    data\n",
    ")\n",
    "num_train_samples = int(hparams['ensemble']['train_frac'] * num_samples)\n",
    "ensemble_train_data = jax.tree_util.tree_map(\n",
    "    lambda a: a[:, :num_train_samples],\n",
    "    shuffled_data\n",
    ")\n",
    "ensemble_valid_data = jax.tree_util.tree_map(\n",
    "    lambda a: a[:, num_train_samples:],\n",
    "    shuffled_data\n",
    ")\n",
    "\n",
    "# Initialize gradient-based optimizer (ADAM)\n",
    "learning_rate = hparams['ensemble']['learning_rate']\n",
    "batch_size = int(hparams['ensemble']['batch_frac'] * num_train_samples)\n",
    "num_batches = num_train_samples // batch_size\n",
    "init_opt, update_opt, get_params = optimizers.adam(learning_rate)\n",
    "opt_states = jax.vmap(init_opt)(ensemble)\n",
    "get_ensemble = jax.jit(jax.vmap(get_params))\n",
    "step_idx = 0\n",
    "best_idx = jnp.zeros(num_models)\n",
    "\n",
    "# Pre-compile before training\n",
    "print('ENSEMBLE TRAINING: Pre-compiling ... ', end='', flush=True)\n",
    "start = time.time()\n",
    "batch = next(epoch(key, ensemble_train_data, batch_size,\n",
    "                    batch_axis=1, ragged=False))\n",
    "_ = step(step_idx, opt_states, hparams['ensemble']['regularizer_l2'],\n",
    "            batch, get_params, update_opt)\n",
    "inf_losses = jnp.broadcast_to(jnp.inf, (num_models,))\n",
    "best_ensemble, best_losses, _ = update_best_ensemble(ensemble,\n",
    "                                                        inf_losses,\n",
    "                                                        ensemble,\n",
    "                                                        ensemble_valid_data)\n",
    "_ = get_ensemble(opt_states)\n",
    "end = time.time()\n",
    "print('done ({:.2f} s)!'.format(end - start))\n",
    "\n",
    "# Do gradient descent\n",
    "for _ in tqdm(range(hparams['ensemble']['num_epochs'])):\n",
    "    key, subkey = jax.random.split(key, 2)\n",
    "    for batch in epoch(subkey, ensemble_train_data, batch_size,\n",
    "                        batch_axis=1, ragged=False):\n",
    "        opt_states = step(step_idx, opt_states,\n",
    "                            hparams['ensemble']['regularizer_l2'],\n",
    "                            batch, get_params, update_opt)\n",
    "        new_ensemble = get_ensemble(opt_states)\n",
    "        old_losses = best_losses\n",
    "        best_ensemble, best_losses, valid_losses = update_best_ensemble(\n",
    "            best_ensemble, best_losses, new_ensemble, batch\n",
    "        )\n",
    "        step_idx += 1\n",
    "        best_idx = jnp.where(old_losses == best_losses,\n",
    "                                best_idx, step_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.flatten_util import ravel_pytree\n",
    "import numpy as np\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def _odeint_ckpt(func, x0, ts, *args):\n",
    "\n",
    "    def scan_fun(carry, t1):\n",
    "        x0, t0, *args = carry\n",
    "        x1 = rk38_step(func, t1 - t0, x0, t0, *args)\n",
    "        carry = (x1, t1, *args)\n",
    "        return carry, x1\n",
    "\n",
    "    ts = jnp.atleast_1d(ts)\n",
    "    init_carry = (x0, ts[0], *args)  # dummy state at same time as `t0`\n",
    "    carry, xs = jax.lax.scan(scan_fun, init_carry, ts)\n",
    "    return xs\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0,))\n",
    "def odeint_ckpt(func, x0, ts, *args):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    flat_x0, unravel = ravel_pytree(x0)\n",
    "\n",
    "    def flat_func(flat_x, t, *args):\n",
    "        x = unravel(flat_x)\n",
    "        dx = func(x, t, *args)\n",
    "        flat_dx, _ = ravel_pytree(dx)\n",
    "        return flat_dx\n",
    "\n",
    "    # Solve in flat form\n",
    "    flat_xs = _odeint_ckpt(flat_func, flat_x0, ts, *args)\n",
    "    xs = jax.vmap(unravel)(flat_xs)\n",
    "    return xs\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(0, 2, 3, 4))\n",
    "def odeint_fixed_step(func, x0, t0, t1, step_size, *args):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    # Use `numpy` for purely static operations on static arguments\n",
    "    # (see: https://github.com/google/jax/issues/5208)\n",
    "    # jdebug.print('{t1_type}', t1_type=type(t1))\n",
    "    num_steps = int(np.maximum(np.abs((t1 - t0)/step_size), 1))\n",
    "\n",
    "    ts = jnp.linspace(t0, t1, num_steps + 1)\n",
    "    xs = odeint_ckpt(func, x0, ts, *args)\n",
    "    return xs, ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize pnorm as 2.00\n",
      "META-TRAINING: Pre-compiling ... "
     ]
    }
   ],
   "source": [
    "# META-TRAINING ##########################################################\n",
    "def ode(z, t, meta_params, pnorm_param, params, reference, prior=prior):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    # jdebug.print(\"t ode: {t_}\", t_=t)\n",
    "    # jdebug.print('begin ode')\n",
    "    # jdebug.print(\"z input: {z_}\", z_=z)\n",
    "    \n",
    "    x, R_flatten, Omega, pA, c = z\n",
    "\n",
    "    if jnp.any(jnp.isnan(x)):\n",
    "        jdebug.print('x: {}', x)\n",
    "    if jnp.any(jnp.isnan(R_flatten)):\n",
    "        jdebug.print('R_flatten: {}', R_flatten)\n",
    "    if jnp.any(jnp.isnan(Omega)):\n",
    "        jdebug.print('Omega: {}', Omega)\n",
    "    if jnp.any(jnp.isnan(pA)):\n",
    "        jdebug.print('pA: {}', pA)\n",
    "    if jnp.any(jnp.isnan(c)):\n",
    "        jdebug.print('c: {}', c)\n",
    "\n",
    "    num_dof = x.size // 2\n",
    "    q, dq = x[:num_dof], x[num_dof:]\n",
    "    r = reference(t)\n",
    "    dr = jax.jacfwd(reference)(t)\n",
    "    ddr = jax.jacfwd(jax.jacfwd(reference))(t)\n",
    "\n",
    "    # Regressor features\n",
    "    y = x\n",
    "    y = jnp.concatenate([y, R_flatten, Omega], axis=0)\n",
    "    for W, b in zip(meta_params['W'], meta_params['b']):\n",
    "        y = jnp.tanh(W@y + b)\n",
    "\n",
    "    # Parameterized control and adaptation gains\n",
    "    gains = jax.tree_util.tree_map(\n",
    "        lambda x: params_to_posdef(x),\n",
    "        meta_params['gains']\n",
    "    )\n",
    "    Λ, K, P = gains['Λ'], gains['K'], gains['P']\n",
    "\n",
    "    qn = 1.1 + pnorm_param['pnorm']**2\n",
    "\n",
    "    # A = jax.scipy.linalg.sqrtm(P) @ (jnp.maximum(jnp.abs(pA), 1e-6 * jnp.ones_like(pA))**(qn-1) * jnp.sign(pA) * (jnp.ones_like(pA) - jnp.isclose(pA, 0, atol=1e-6)))\n",
    "    # Previous implementation P size: feature_size x feature_size\n",
    "    A = (jnp.maximum(jnp.abs(pA), 1e-6 * jnp.ones_like(pA))**(qn-1) * jnp.sign(pA) * (jnp.ones_like(pA) - jnp.isclose(pA, 0, atol=1e-6))) @ P\n",
    "    \n",
    "\n",
    "    # Auxiliary signals\n",
    "    e, de = q - r, dq - dr\n",
    "    v, dv = dr - Λ@e, ddr - Λ@de\n",
    "    s = de + Λ@e\n",
    "\n",
    "    # Controller and adaptation law\n",
    "    H, C, g, B = prior(q, dq)\n",
    "    f_ext_hat = A@y\n",
    "    τ = H@dv + C@v + g - f_ext_hat - K@s\n",
    "    u_d = jnp.linalg.solve(B, τ)\n",
    "    # dA = jax.scipy.linalg.sqrtm(P) @ jnp.outer(s, y)\n",
    "    dA = jnp.outer(s, y) @ P\n",
    "\n",
    "    R = R_flatten.reshape((3,3))\n",
    "\n",
    "    f_d = jnp.linalg.norm(u_d)\n",
    "    b_3d = -u_d / jnp.linalg.norm(u_d)\n",
    "    b_1d = jnp.array([1, 0, 0])\n",
    "    cross = jnp.cross(b_3d, b_1d)\n",
    "    b_2d = cross / jnp.linalg.norm(cross)\n",
    "\n",
    "    R_d = jnp.column_stack((jnp.cross(b_2d, b_3d), b_2d, b_3d))\n",
    "\n",
    "    Omega_d = jnp.array([0, 0, 0])\n",
    "    dOmega_d = jnp.array([0, 0, 0])\n",
    "\n",
    "    k_R = jnp.array([1400.0, 1400.0, 1260.0])\n",
    "    k_Omega = jnp.array([330.0, 330.0, 300.0])\n",
    "    J = jnp.diag(jnp.array([0.03, 0.03, 0.09]))\n",
    "\n",
    "    e_R = 0.5 * vee(R_d.T@R - R.T@R_d)\n",
    "    e_Omega = Omega - R.T@R_d@Omega_d\n",
    "\n",
    "    M = - k_R*e_R \\\n",
    "        - k_Omega*e_Omega \\\n",
    "        + jnp.cross(Omega, J@Omega) \\\n",
    "        - J@(hat(Omega)@R.T@R_d@Omega_d - R.T@R_d@dOmega_d)\n",
    "\n",
    "    dOmega = jax.scipy.linalg.solve(J, M - jnp.cross(Omega, J@Omega), assume_a='pos')\n",
    "    dR = R@hat(Omega)\n",
    "    dR_flatten = dR.flatten()\n",
    "\n",
    "    e_3 = jnp.array([0, 0, 1])\n",
    "    u = -f_d*R@e_3\n",
    "\n",
    "    # Apply control to \"true\" dynamics\n",
    "    f_ext = x\n",
    "    f_ext = jnp.concatenate([f_ext, R_flatten, Omega], axis=0)\n",
    "    for W, b in zip(params['W'], params['b']):\n",
    "        f_ext = jnp.tanh(W@f_ext + b)\n",
    "    f_ext = params['A'] @ f_ext\n",
    "    ddq = jax.scipy.linalg.solve(H, u + f_ext - C@dq - g, assume_a='pos')\n",
    "    dx = jnp.concatenate((dq, ddq))\n",
    "\n",
    "    # Estimation loss\n",
    "    # chol_P = params_to_cholesky(meta_params['gains']['P'])\n",
    "    # f_error = f_hat - f\n",
    "    # loss_est = f_error@jax.scipy.linalg.cho_solve((chol_P, True),\n",
    "    #                                               f_error)\n",
    "\n",
    "    # Integrated cost terms\n",
    "    dc = jnp.array([\n",
    "        e@e + de@de,                # tracking loss\n",
    "        u_d@u_d,                        # control loss\n",
    "        (f_ext_hat - f_ext)@(f_ext_hat - f_ext),    # estimation loss\n",
    "    ])\n",
    "\n",
    "    # Assemble derivatives\n",
    "    dz = (dx, dR_flatten, dOmega, dA, dc)\n",
    "    # jdebug.print('dx: {}', dx)\n",
    "    return dz\n",
    "\n",
    "\n",
    "# Simulate adaptive control loop on each model in the ensemble\n",
    "def ensemble_sim(meta_params, pnorm_param, ensemble_params, reference, T, dt, ode=ode):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    # Initial conditions\n",
    "    r0 = reference(0.)\n",
    "    dr0 = jax.jacfwd(reference)(0.)\n",
    "    num_dof = r0.size\n",
    "    num_features = meta_params['W'][-1].shape[0]\n",
    "    x0 = jnp.concatenate((r0, dr0))\n",
    "    R_flatten0 = jnp.zeros(9)\n",
    "    Omega0 = jnp.zeros(3)\n",
    "    A0 = jnp.zeros((num_dof, num_features))\n",
    "    c0 = jnp.zeros(3)\n",
    "    z0 = (x0, R_flatten0, Omega0, A0, c0)\n",
    "\n",
    "    # Integrate the adaptive control loop using the meta-model\n",
    "    # and EACH model in the ensemble along the same reference\n",
    "    in_axes = (None, None, None, None, None, None, None, 0)\n",
    "    ode = partial(ode, reference=reference)\n",
    "    # jdebug.print('before ode')\n",
    "    # jdebug.print('x0: {}', x0.shape)\n",
    "    # jdebug.print('R_flatten0: {}', R_flatten0.shape)\n",
    "    # jdebug.print('Omega0: {}', Omega0.shape)\n",
    "    # jdebug.print('A0: {}', A0.shape)\n",
    "    # jdebug.print('c0: {}', c0.shape)\n",
    "    # jdebug.print('z0: {}', len(z0))\n",
    "    z, t = jax.vmap(odeint_fixed_step, in_axes)(ode, z0, 0., T, dt,\n",
    "                                                meta_params, pnorm_param,\n",
    "                                                ensemble_params)\n",
    "    jdebug.print('after ode')\n",
    "    x, R_flatten, Omega, A, c = z\n",
    "    # jdebug.print('x: {}', x.shape)\n",
    "    # jdebug.print('R_flatten: {}', R_flatten.shape)\n",
    "    # jdebug.print('Omega: {}', Omega.shape)\n",
    "    # jdebug.print('A: {}', A.shape)\n",
    "    # jdebug.print('c: {}', c.shape)\n",
    "    # jdebug.print('z output: {}', len(z))\n",
    "    return t, x, R_flatten, Omega, A, c\n",
    "\n",
    "# Initialize meta-model parameters\n",
    "num_hlayers = hparams['meta']['num_hlayers']\n",
    "hdim = hparams['meta']['hdim']\n",
    "if num_hlayers >= 1:\n",
    "    shapes = [(hdim, param_dim), ] + (num_hlayers-1)*[(hdim, hdim), ]\n",
    "else:\n",
    "    shapes = []\n",
    "key, *subkeys = jax.random.split(key, 1 + 2*num_hlayers + 3)\n",
    "subkeys_W = subkeys[:num_hlayers]\n",
    "subkeys_b = subkeys[num_hlayers:-3]\n",
    "subkeys_gains = subkeys[-3:]\n",
    "meta_params = {\n",
    "    # hidden layer weights\n",
    "    'W': [0.1*jax.random.normal(subkeys_W[i], shapes[i])\n",
    "            for i in range(num_hlayers)],\n",
    "    # hidden layer biases\n",
    "    'b': [0.1*jax.random.normal(subkeys_b[i], (shapes[i][0],))\n",
    "            for i in range(num_hlayers)],\n",
    "    'gains': {  # vectorized control and adaptation gains\n",
    "        'Λ': 0.1*jax.random.normal(subkeys_gains[0],\n",
    "                                    ((num_dof*(num_dof + 1)) // 2,)),\n",
    "        'K': 0.1*jax.random.normal(subkeys_gains[1],\n",
    "                                    ((num_dof*(num_dof + 1)) // 2,)),\n",
    "        'P': 0.1*jax.random.normal(subkeys_gains[2],\n",
    "                                    ((hdim*(hdim + 1)) // 2,)),\n",
    "        # 'P': 0.1*jax.random.normal(subkeys_gains[2],\n",
    "                                #    ((num_dof*(num_dof + 1)) // 2,)),\n",
    "    },\n",
    "}\n",
    "# In the bash script, we always specify p-norm desried initial values\n",
    "# Note that the program always uses the q_bar parameter as the p-norm parameterization\n",
    "# However, the printing function should log the final results in p-norm\n",
    "pnorm_param = {'pnorm': convert_p_qbar(args.pnorm_init)}\n",
    "print(\"Initialize pnorm as {:.2f}\".format(convert_qbar_p(pnorm_param['pnorm'])))\n",
    "\n",
    "# Initialize spline coefficients for each reference trajectory\n",
    "num_refs = hparams['meta']['num_refs']\n",
    "key, *subkeys = jax.random.split(key, 1 + num_refs)\n",
    "subkeys = jnp.vstack(subkeys)\n",
    "in_axes = (0, None, None, None, None, None, None, None, None)\n",
    "min_ref = jnp.asarray(hparams['meta']['min_ref'])\n",
    "max_ref = jnp.asarray(hparams['meta']['max_ref'])\n",
    "t_knots, knots, coefs = jax.vmap(random_ragged_spline, in_axes)(\n",
    "    subkeys,\n",
    "    hparams['meta']['T'],\n",
    "    hparams['meta']['num_knots'],\n",
    "    hparams['meta']['poly_orders'],\n",
    "    hparams['meta']['deriv_orders'],\n",
    "    jnp.asarray(hparams['meta']['min_step']),\n",
    "    jnp.asarray(hparams['meta']['max_step']),\n",
    "    0.7*min_ref,\n",
    "    0.7*max_ref,\n",
    ")\n",
    "# x_coefs, y_coefs, θ_coefs = coefs\n",
    "# x_knots, y_knots, θ_knots = knots\n",
    "r_knots = jnp.dstack(knots)\n",
    "\n",
    "# Simulate the adaptive control loop for each model in the ensemble and\n",
    "# each reference trajectory (i.e., spline coefficients)\n",
    "@partial(jax.vmap, in_axes=(None, None, None, 0, 0, None, None))\n",
    "def simulate(meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt, min_ref=min_ref, max_ref=max_ref):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    # Define a reference trajectory in terms of spline coefficients\n",
    "    def reference(t):\n",
    "        r = jnp.array([spline(t, t_knots, c) for c in coefs])\n",
    "        r = jnp.clip(r, min_ref, max_ref)\n",
    "        return r\n",
    "    t, x, R_flatten, Omega, A, c = ensemble_sim(meta_params, pnorm_param, ensemble_params,\n",
    "                                reference, T, dt)\n",
    "    return t, x, R_flatten, Omega, A, c\n",
    "\n",
    "@partial(jax.jit, static_argnums=(5, 6))\n",
    "def loss(meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt,\n",
    "            regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P):\n",
    "    \"\"\"TODO: docstring.\"\"\"\n",
    "    # Simulate on each model for each reference trajectory\n",
    "    t, x, R_flatten, Omega, A, c = simulate(meta_params, pnorm_param, ensemble_params, t_knots,\n",
    "                            coefs, T, dt)\n",
    "\n",
    "    # Sum final costs over reference trajectories and ensemble models\n",
    "    # Note `c` has shape (`num_refs`, `num_models`, `T // dt`, 3)\n",
    "    c_final = jnp.sum(c[:, :, -1, :], axis=(0, 1))\n",
    "\n",
    "    # Form a composite loss by weighting the different cost integrals,\n",
    "    # and normalizing by the number of models, number of reference\n",
    "    # trajectories, and time horizon\n",
    "    num_refs = c.shape[0]\n",
    "    num_models = c.shape[1]\n",
    "    normalizer = T * num_refs * num_models\n",
    "    tracking_loss, control_loss, estimation_loss = c_final\n",
    "    reg_P_penalty = jnp.linalg.norm(meta_params['gains']['P'])**2\n",
    "    l2_penalty = tree_normsq((meta_params['W'], meta_params['b']))\n",
    "    # regularization on P Frobenius norm shouldn't be normalized\n",
    "    loss = (tracking_loss\n",
    "            + regularizer_ctrl*control_loss\n",
    "            + regularizer_error*estimation_loss\n",
    "            + regularizer_l2*l2_penalty\n",
    "            ) / normalizer + regularizer_P * reg_P_penalty\n",
    "    \n",
    "\n",
    "    aux = {\n",
    "        # for each model in ensemble\n",
    "        'tracking_loss':   jnp.sum(c[:, :, -1, 0], axis=0) / num_refs,\n",
    "        'control_loss':    jnp.sum(c[:, :, -1, 1], axis=0) / num_refs,\n",
    "        'estimation_loss': jnp.sum(c[:, :, -1, 2], axis=0) / num_refs,\n",
    "        'l2_penalty':      l2_penalty,\n",
    "        'reg_P_penalty': reg_P_penalty,\n",
    "        'eigs_Λ':\n",
    "            jnp.diag(params_to_cholesky(meta_params['gains']['Λ']))**2,\n",
    "        'eigs_K':\n",
    "            jnp.diag(params_to_cholesky(meta_params['gains']['K']))**2,\n",
    "        'eigs_P':\n",
    "            jnp.diag(params_to_cholesky(meta_params['gains']['P']))**2,\n",
    "        'pnorm': pnorm_param['pnorm']\n",
    "    }\n",
    "    return loss, aux\n",
    "\n",
    "# Shuffle and split ensemble into training and validation sets\n",
    "train_frac = hparams['meta']['train_frac']\n",
    "num_train_models = int(train_frac * num_models)\n",
    "key, subkey = jax.random.split(key, 2)\n",
    "model_idx = jax.random.permutation(subkey, num_models)\n",
    "train_model_idx = model_idx[:num_train_models]\n",
    "valid_model_idx = model_idx[num_train_models:]\n",
    "train_ensemble = jax.tree_util.tree_map(lambda x: x[train_model_idx],\n",
    "                                        best_ensemble)\n",
    "valid_ensemble = jax.tree_util.tree_map(lambda x: x[valid_model_idx],\n",
    "                                        best_ensemble)\n",
    "\n",
    "# Split reference trajectories into training and validation sets\n",
    "num_train_refs = int(train_frac * num_refs)\n",
    "train_t_knots = jax.tree_util.tree_map(lambda a: a[:num_train_refs],\n",
    "                                        t_knots)\n",
    "train_coefs = jax.tree_util.tree_map(lambda a: a[:num_train_refs], coefs)\n",
    "valid_t_knots = jax.tree_util.tree_map(lambda a: a[num_train_refs:],\n",
    "                                        t_knots)\n",
    "valid_coefs = jax.tree_util.tree_map(lambda a: a[num_train_refs:], coefs)\n",
    "\n",
    "# Initialize gradient-based optimizer (ADAM)\n",
    "learning_rate = hparams['meta']['learning_rate']\n",
    "init_opt, update_opt, get_params = optimizers.adam(learning_rate)\n",
    "# Update meta_params and pnorm_param separately\n",
    "opt_meta = init_opt(meta_params)\n",
    "opt_pnorm = init_opt(pnorm_param)\n",
    "step_meta_idx = 0\n",
    "step_pnorm_idx = 0\n",
    "best_idx_meta = 0\n",
    "best_idx_pnorm = 0\n",
    "best_loss = jnp.inf\n",
    "best_meta_params = meta_params\n",
    "best_pnorm_param = pnorm_param\n",
    "\n",
    "\n",
    "@partial(jax.jit, static_argnums=(6, 7))\n",
    "def step_meta(idx, opt_state, pnorm_param, ensemble_params, t_knots, coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P):\n",
    "    \"\"\"This function only updates the meta_params in an iteration\"\"\"\n",
    "    meta_params = get_params(opt_state)\n",
    "    grads, aux = jax.grad(loss, argnums=0, has_aux=True)(\n",
    "        meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt,\n",
    "        regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P\n",
    "    )\n",
    "    opt_state = update_opt(idx, grads, opt_state)\n",
    "    return opt_state, aux, grads\n",
    "\n",
    "@partial(jax.jit, static_argnums=(6, 7))\n",
    "def step_pnorm(idx, meta_params, opt_state, ensemble_params, t_knots, coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P):\n",
    "    \"\"\"This function only updates the meta_params in an iteration\"\"\"\n",
    "    pnorm_param = get_params(opt_state)\n",
    "    grads, aux = jax.grad(loss, argnums=1, has_aux=True)(\n",
    "        meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt,\n",
    "        regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P\n",
    "    )\n",
    "    opt_state = update_opt(idx, grads, opt_state)\n",
    "    return opt_state, aux, grads\n",
    "\n",
    "# Pre-compile before training\n",
    "print('META-TRAINING: Pre-compiling ... ', end='', flush=True)\n",
    "dt = hparams['meta']['dt']\n",
    "T = hparams['meta']['T']\n",
    "regularizer_l2 = hparams['meta']['regularizer_l2']\n",
    "regularizer_ctrl = hparams['meta']['regularizer_ctrl']\n",
    "regularizer_error = hparams['meta']['regularizer_error']\n",
    "regularizer_P = hparams['meta']['regularizer_P']\n",
    "start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x0: (Array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  6.6536113e-06,\n",
      "       -2.9316909e-06,  2.4635467e-06], dtype=float32), Array([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), Array([0., 0., 0.], dtype=float32), Array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],      dtype=float32), Array([0., 0., 0.], dtype=float32))\n",
      "x0: (Array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  9.6598706e-06,\n",
      "        2.9255487e-05, -1.5986208e-05], dtype=float32), Array([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), Array([0., 0., 0.], dtype=float32), Array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],      dtype=float32), Array([0., 0., 0.], dtype=float32))\n",
      "x0: (Array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  5.6182012e-05,\n",
      "       -1.9321415e-05, -1.9509538e-05], dtype=float32), Array([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), Array([0., 0., 0.], dtype=float32), Array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],      dtype=float32), Array([0., 0., 0.], dtype=float32))\n",
      "x0: (Array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -7.8831636e-06,\n",
      "       -5.8060245e-08, -4.0580875e-07], dtype=float32), Array([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), Array([0., 0., 0.], dtype=float32), Array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],      dtype=float32), Array([0., 0., 0.], dtype=float32))\n",
      "x0: (Array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  2.7732547e-06,\n",
      "        8.6332502e-06, -1.0038553e-06], dtype=float32), Array([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), Array([0., 0., 0.], dtype=float32), Array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],      dtype=float32), Array([0., 0., 0.], dtype=float32))\n",
      "x0: (Array([ 0.000000e+00,  0.000000e+00,  0.000000e+00, -8.321280e-05,\n",
      "        2.916966e-05,  3.721297e-05], dtype=float32), Array([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), Array([0., 0., 0.], dtype=float32), Array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],      dtype=float32), Array([0., 0., 0.], dtype=float32))\n",
      "x0: (Array([ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, -1.6059571e-04,\n",
      "        1.0721945e-04,  8.9901027e-05], dtype=float32), Array([0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), Array([0., 0., 0.], dtype=float32), Array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],      dtype=float32), Array([0., 0., 0.], dtype=float32))\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'xs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[221], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mdisable_jit():\n\u001b[0;32m----> 2\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[43mstep_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpnorm_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ensemble\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_t_knots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_coefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularizer_l2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularizer_ctrl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularizer_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularizer_P\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m _ \u001b[38;5;241m=\u001b[39m step_pnorm(\u001b[38;5;241m0\u001b[39m, meta_params, opt_pnorm, train_ensemble, train_t_knots, train_coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P)\n\u001b[1;32m      4\u001b[0m _ \u001b[38;5;241m=\u001b[39m loss(meta_params, pnorm_param, valid_ensemble, valid_t_knots, valid_coefs, T, dt,\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m, \u001b[38;5;241m0.\u001b[39m)\n",
      "Cell \u001b[0;32mIn[220], line 316\u001b[0m, in \u001b[0;36mstep_meta\u001b[0;34m(idx, opt_state, pnorm_param, ensemble_params, t_knots, coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"This function only updates the meta_params in an iteration\"\"\"\u001b[39;00m\n\u001b[1;32m    315\u001b[0m meta_params \u001b[38;5;241m=\u001b[39m get_params(opt_state)\n\u001b[0;32m--> 316\u001b[0m grads, aux \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpnorm_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensemble_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_knots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43mregularizer_l2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularizer_ctrl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularizer_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregularizer_P\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m opt_state \u001b[38;5;241m=\u001b[39m update_opt(idx, grads, opt_state)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m opt_state, aux, grads\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[220], line 235\u001b[0m, in \u001b[0;36mloss\u001b[0;34m(meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"TODO: docstring.\"\"\"\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;66;03m# Simulate on each model for each reference trajectory\u001b[39;00m\n\u001b[0;32m--> 235\u001b[0m t, x, R_flatten, Omega, A, c \u001b[38;5;241m=\u001b[39m \u001b[43msimulate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpnorm_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensemble_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_knots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcoefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;66;03m# Sum final costs over reference trajectories and ensemble models\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# Note `c` has shape (`num_refs`, `num_models`, `T // dt`, 3)\u001b[39;00m\n\u001b[1;32m    240\u001b[0m c_final \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msum(c[:, :, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[220], line 226\u001b[0m, in \u001b[0;36msimulate\u001b[0;34m(meta_params, pnorm_param, ensemble_params, t_knots, coefs, T, dt, min_ref, max_ref)\u001b[0m\n\u001b[1;32m    224\u001b[0m     r \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mclip(r, min_ref, max_ref)\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m r\n\u001b[0;32m--> 226\u001b[0m t, x, R_flatten, Omega, A, c \u001b[38;5;241m=\u001b[39m \u001b[43mensemble_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeta_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpnorm_param\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensemble_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mreference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t, x, R_flatten, Omega, A, c\n",
      "Cell \u001b[0;32mIn[220], line 146\u001b[0m, in \u001b[0;36mensemble_sim\u001b[0;34m(meta_params, pnorm_param, ensemble_params, reference, T, dt, ode)\u001b[0m\n\u001b[1;32m    138\u001b[0m ode \u001b[38;5;241m=\u001b[39m partial(ode, reference\u001b[38;5;241m=\u001b[39mreference)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m# jdebug.print('before ode')\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# jdebug.print('x0: {}', x0.shape)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m# jdebug.print('R_flatten0: {}', R_flatten0.shape)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m# jdebug.print('c0: {}', c0.shape)\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# jdebug.print('z0: {}', len(z0))\u001b[39;00m\n\u001b[0;32m--> 146\u001b[0m z, t \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43modeint_fixed_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmeta_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpnorm_param\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mensemble_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m jdebug\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter ode\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    150\u001b[0m x, R_flatten, Omega, A, c \u001b[38;5;241m=\u001b[39m z\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[219], line 51\u001b[0m, in \u001b[0;36modeint_fixed_step\u001b[0;34m(func, x0, t0, t1, step_size, *args)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     jdebug\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx0: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, x0)\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mxs\u001b[49m, ts\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'xs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "with jax.disable_jit():\n",
    "    _ = step_meta(0, opt_meta, pnorm_param, train_ensemble, train_t_knots, train_coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P)\n",
    "_ = step_pnorm(0, meta_params, opt_pnorm, train_ensemble, train_t_knots, train_coefs, T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P)\n",
    "_ = loss(meta_params, pnorm_param, valid_ensemble, valid_t_knots, valid_coefs, T, dt,\n",
    "            0., 0., 0., 0.)\n",
    "end = time.time()\n",
    "print('done ({:.2f} s)! Now training ...'.format(\n",
    "        end - start))\n",
    "\n",
    "# Record pnorm and training loss history\n",
    "train_lossaux_history = []\n",
    "valid_loss_history = []\n",
    "pnorm_history = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Do gradient descent\n",
    "for i in tqdm(range(hparams['meta']['num_steps'])):\n",
    "    opt_meta, train_aux_meta, grads_meta = step_meta(\n",
    "        step_meta_idx, opt_meta, pnorm_param, train_ensemble, train_t_knots, train_coefs,\n",
    "        T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P\n",
    "    )\n",
    "    # print(train_aux_meta)\n",
    "    new_meta_params = get_params(opt_meta)\n",
    "\n",
    "    # Update p-norm parameter\n",
    "    # The i+1 is to make sure not to update p-norm at step 0\n",
    "    if (i+1) % hparams['meta']['p_freq'] == 0:\n",
    "        opt_pnorm, train_aux_pnorm, grads_pnorm = step_pnorm(\n",
    "            step_pnorm_idx, new_meta_params, opt_pnorm, train_ensemble, train_t_knots, train_coefs,\n",
    "            T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P\n",
    "        )\n",
    "        step_pnorm_idx += 1\n",
    "        # jdebug.print('{grad_pnorm}', grad_pnorm=grads_pnorm)\n",
    "        # jdebug.print('{meta_grad}', meta_grad=grads_meta)\n",
    "        print(\"Update p-norm to {:.2f} at step {:d}\".format(convert_qbar_p(get_params(opt_pnorm)['pnorm']), step_meta_idx))\n",
    "        pnorm_history.append(convert_qbar_p(get_params(opt_pnorm)['pnorm']))\n",
    "        train_lossaux_history.append(train_aux_pnorm)\n",
    "    else:\n",
    "        train_lossaux_history.append(train_aux_meta)\n",
    "\n",
    "    new_pnorm_param = get_params(opt_pnorm)\n",
    "        \n",
    "    valid_loss, valid_aux = loss(\n",
    "        new_meta_params, new_pnorm_param, valid_ensemble, valid_t_knots, valid_coefs,\n",
    "        T, dt, 0., 0., 0., 0.\n",
    "    )\n",
    "    train_loss, train_aux = loss(\n",
    "        new_meta_params, new_pnorm_param, train_ensemble, train_t_knots, train_coefs,\n",
    "        T, dt, regularizer_l2, regularizer_ctrl, regularizer_error, regularizer_P)\n",
    "    \n",
    "    valid_loss_history.append(valid_loss)\n",
    "\n",
    "    # Only update best_meta_params when the loss is decreasing\n",
    "    if valid_loss < best_loss:\n",
    "        best_meta_params = new_meta_params\n",
    "        best_pnorm_param = new_pnorm_param\n",
    "        best_loss = valid_loss\n",
    "        best_idx_meta = step_meta_idx\n",
    "        best_idx_pnorm = step_pnorm_idx\n",
    "    step_meta_idx += 1\n",
    "\n",
    "# Save hyperparameters, ensemble, model, and controller\n",
    "output_name = \"seed={:d}_M={:d}_E={:d}_pinit={:.2f}_pfreq={:.0f}_regP={:.4f}\".format(hparams['seed'], num_models, args.meta_epochs, args.pnorm_init, hparams['meta']['p_freq'], hparams['meta']['regularizer_P'])\n",
    "results = {\n",
    "    'best_step_meta': best_idx_meta,\n",
    "    'best_step_pnorm': best_idx_pnorm,\n",
    "    'hparams': hparams,\n",
    "    'ensemble': best_ensemble,\n",
    "    'model': {\n",
    "        'W': best_meta_params['W'],\n",
    "        'b': best_meta_params['b'],\n",
    "    },\n",
    "    'controller': best_meta_params['gains'],\n",
    "    'pnorm': convert_qbar_p(best_pnorm_param['pnorm']),\n",
    "    'regP': hparams['meta']['regularizer_P'],\n",
    "    'train_lossaux_history': train_lossaux_history,\n",
    "    'valid_loss_history': valid_loss_history,\n",
    "    'pnorm_history': pnorm_history\n",
    "}\n",
    "output_dir = os.path.join('train_results', args.output_dir)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "output_path = os.path.join(output_dir, output_name + '.pkl')\n",
    "with open(output_path, 'wb') as file:\n",
    "    pickle.dump(results, file)\n",
    "\n",
    "end = time.time()\n",
    "print(\"Meta-training completes with p-norm chosen as {:.2f}\".format(results['pnorm']))\n",
    "print('done ({:.2f} s)! Best step index for meta params: {}'.format(end - start, best_idx_meta))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
